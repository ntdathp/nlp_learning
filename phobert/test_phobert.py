#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse, re, numpy as np, torch, unicodedata, string
from transformers import AutoTokenizer, AutoModelForSequenceClassification

LABELS_5 = ["very_negative","negative","neutral","positive","very_positive"]
ID2LBL = {i:l for i,l in enumerate(LABELS_5)}

EMO_POS = ["ü§©","ü•∞","üòç","‚ù§Ô∏è","üëç","üòé","üëå","‚ú®","üî•","üíØ"]
EMO_NEG = ["üò±","üò°","ü§¨","üí©","üëé","üò§","üòû","üò≠"]

TEENCODE_MAP = {
    r"\bko\b": "kh√¥ng", r"\bk\b": "kh√¥ng", r"\bkh\b": "kh√¥ng",
    r"\bkhong\b": "kh√¥ng", r"\bhong\b": "kh√¥ng",
    r"\bvl\b": "r·∫•t", r"\bvcl\b": "r·∫•t",
    r"\bqa\b": "qu√°", r"\bqua\b": "qu√°", r"\bhqa\b": "h∆°i qu√°",
    r"\bok+\b": "ok", r"\boke+\b": "ok", r"\bokeee\b": "ok",
}

ACCENT_MAP = {
    "may":"m√°y","nong":"n√≥ng","mat":"m√°t","on":"·ªïn","kem":"k√©m","tot":"t·ªët","te":"t·ªá",
    "thatvong":"th·∫•t v·ªçng","tuyetvoi":"tuy·ªát v·ªùi","tuyet":"tuy·ªát",
    "hai_long":"h√†i l√≤ng","hai long":"h√†i l√≤ng",
    "dang":"ƒë√°ng","gia":"gi√°","dat":"ƒë·∫Øt","re":"r·∫ª","ton":"t·ªën","tien":"ti·ªÅn","mua":"mua",
    "pin":"pin","am":"√¢m","thanh":"thanh","am thanh":"√¢m thanh","am_thanh":"√¢m thanh",
    "dong":"ƒë√≥ng","goi":"g√≥i","dong goi":"ƒë√≥ng g√≥i","donggoi":"ƒë√≥ng g√≥i",
    "nhanh":"nhanh","cham":"ch·∫≠m","tre":"tr·ªÖ","lau":"l√¢u","som":"s·ªõm","ben":"b·ªÅn","yeu":"y·∫øu",
    "man":"m√†n","hinh":"h√¨nh","man hinh":"m√†n h√¨nh","man_hinh":"m√†n h√¨nh",
    "sac":"s·∫°c","sac du phong":"s·∫°c d·ª± ph√≤ng","sacduphong":"s·∫°c d·ª± ph√≤ng",
    "nhiet":"nhi·ªát","do":"ƒë·ªô","nhiet do":"nhi·ªát ƒë·ªô","nhiet_do":"nhi·ªát ƒë·ªô",
    "dich vu":"d·ªãch v·ª•","dichvu":"d·ªãch v·ª•","ho tro":"h·ªó tr·ª£","ho_tro":"h·ªó tr·ª£",
    "phan hoi":"ph·∫£n h·ªìi","phanhoi":"ph·∫£n h·ªìi",
    "chat luong":"ch·∫•t l∆∞·ª£ng","chatluong":"ch·∫•t l∆∞·ª£ng",
    "trai nghiem":"tr·∫£i nghi·ªám","trai_nghiem":"tr·∫£i nghi·ªám",
    "cau tha":"c·∫©u th·∫£","cau_tha":"c·∫©u th·∫£",
    "dep":"ƒë·∫πp","xau":"x·∫•u","xuat sac":"xu·∫•t s·∫Øc","xuat_sac":"xu·∫•t s·∫Øc",
    "qua":"qu√°","rat":"r·∫•t","cuc":"c·ª±c","cuc ky":"c·ª±c k·ª≥","cuc_ky":"c·ª±c k·ª≥",
    "khong":"kh√¥ng","khong nen":"kh√¥ng n√™n",
    "dang tien":"ƒë√°ng ti·ªÅn","dang gia":"ƒë√°ng gi√°",
    "giong":"gi·ªëng","mo ta":"m√¥ t·∫£","mo_ta":"m√¥ t·∫£",
    "binh thuong":"b√¨nh th∆∞·ªùng","binh_thuong":"b√¨nh th∆∞·ªùng",
    "thich":"th√≠ch","rat thich":"r·∫•t th√≠ch","rat_thich":"r·∫•t th√≠ch","ung":"∆∞ng",
}

BIGRAM_HINTS = {
    ("r·∫•t","t·ªët"): 2.0, ("r·∫•t","th·∫•t"): 1.5, ("r·∫•t","ƒë√°ng"): 1.5,
    ("m√°y","n√≥ng"): 2.0, ("t·ªën","ti·ªÅn"): 2.0, ("√¢m","thanh"): 2.0,
    ("ƒë√≥ng","g√≥i"): 2.0, ("m√†n","h√¨nh"): 2.0, ("ch·∫•t","l∆∞·ª£ng"): 2.0,
    ("tr·∫£i","nghi·ªám"): 2.0, ("ph·∫£n","h·ªìi"): 1.5, ("r·∫•t","th√≠ch"): 1.5,
}

def normalize_text(s: str) -> str:
    s = unicodedata.normalize("NFC", str(s).strip())
    for e in EMO_POS: s = s.replace(e, " EMO_POS ")
    for e in EMO_NEG: s = s.replace(e, " EMO_NEG ")
    s = s.lower()
    for pat, rep in TEENCODE_MAP.items():
        s = re.sub(pat, rep, s, flags=re.IGNORECASE)
    repl = {
        "vl":"r·∫•t","okeee":"ok","∆∞ng":"r·∫•t th√≠ch","si√™u si√™u":"r·∫•t",
        "si√™u th·∫•t v·ªçng":"r·∫•t th·∫•t v·ªçng","m√£i ƒë·ªânh":"r·∫•t t·ªët",
        "best of best":"r·∫•t t·ªët","best choice":"r·∫•t t·ªët","ƒë·ªânh c·ªßa ch√≥p":"r·∫•t t·ªët",
    }
    for k,v in repl.items():
        s = re.sub(rf"\b{re.escape(k)}\b", v, s, flags=re.IGNORECASE)
    return re.sub(r"\s+"," ", s).strip()

def maybe_segment(text, use_seg=False):
    if not use_seg: return text
    try:
        from underthesea import word_tokenize
        return word_tokenize(text, format="text")
    except Exception:
        return text

def approx_diacritic_ratio(s: str) -> float:
    vowels_with_tone = "√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë"
    s_low = s.lower()
    cnt_diac = sum(ch in vowels_with_tone for ch in s_low)
    cnt_letters = sum(ch.isalpha() for ch in s_low)
    return (cnt_diac / max(1, cnt_letters))

def strip_accents_simple(s: str) -> str:
    s = unicodedata.normalize("NFD", s)
    s = s.replace("ƒë","d").replace("ƒê","D")
    s = "".join(ch for ch in s if unicodedata.category(ch) != "Mn")
    return unicodedata.normalize("NFC", s)

def _choose_variant(token_lc: str, prev_lc: str = None):
    cand = ACCENT_MAP.get(token_lc)
    if cand and prev_lc:
        score = 1.0 + BIGRAM_HINTS.get((prev_lc, cand.split()[0]), 0.0)
        return cand, score
    if cand:
        return cand, 1.0
    return None, 0.0

def restore_diacritics(text: str) -> str:
    tokens = text.split(" ")
    out_tokens, prev_norm = [], None
    for tok in tokens:
        # t√°ch punctuation ƒë·∫ßu/cu·ªëi
        prefix, suffix, core = "", "", tok
        while core and core[0] in string.punctuation:
            prefix += core[0]; core = core[1:]
        while core and core[-1] in string.punctuation:
            suffix = core[-1] + suffix; core = core[:-1]
        if not core:
            out_tokens.append(prefix + suffix); prev_norm = None; continue
        base = strip_accents_simple(core.lower())
        best, _ = _choose_variant(base, prev_norm)
        replaced = best if best else core
        if core.isupper():
            replaced = replaced.upper()
        elif core[0].isupper():
            replaced = replaced[0].upper() + replaced[1:]
        out_tokens.append(prefix + replaced + suffix)
        prev_norm = replaced.split()[0].lower()
    return " ".join(out_tokens)

def softmax(x):
    x = x - np.max(x, keepdims=True)
    e = np.exp(x)
    return e / np.sum(e, keepdims=True)

def main():
    ap = argparse.ArgumentParser(description="Quick inference cho 1 c√¢u (PhoBERT 5 l·ªõp).")
    ap.add_argument("--model_dir", default="/home/dat/llm_ws/phobert_5cls_clean")
    ap.add_argument("--text", required=True)
    ap.add_argument("--max_len", type=int, default=160)
    ap.add_argument("--use_seg", action="store_true")
    ap.add_argument("--normalize", action="store_true", default=True)
    # Auto-restore: b·∫≠t m·∫∑c ƒë·ªãnh; c√≥ th·ªÉ t·∫Øt b·∫±ng --no_auto_restore
    ap.add_argument("--no_auto_restore", action="store_true",
                    help="T·∫Øt t·ª± ƒë·ªông kh√¥i ph·ª•c d·∫•u khi ph√°t hi·ªán thi·∫øu d·∫•u.")
    ap.add_argument("--restore_threshold", type=float, default=0.03,
                    help="Ng∆∞·ª°ng t·ª∑ l·ªá k√Ω t·ª± c√≥ d·∫•u ƒë·ªÉ k√≠ch ho·∫°t kh√¥i ph·ª•c.")
    ap.add_argument("--neutral_penalty", type=float, default=0.0)
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"
    tok = AutoTokenizer.from_pretrained(args.model_dir, use_fast=False)
    mdl = AutoModelForSequenceClassification.from_pretrained(args.model_dir).to(device).eval()

    raw = args.text
    s = normalize_text(raw) if args.normalize else raw

    # T·ª∞ ƒê·ªòNG ph√°t hi·ªán & kh√¥i ph·ª•c d·∫•u (tr·ª´ khi t·∫Øt b·∫±ng --no_auto_restore)
    if not args.no_auto_restore and approx_diacritic_ratio(s) < args.restore_threshold:
        restored = restore_diacritics(s)
        print(f"[INFO] Auto-restore diacritics ‚Üí \"{restored}\"")
        s = restored

    s = maybe_segment(s, use_seg=args.use_seg)

    with torch.no_grad():
        enc = tok([s], truncation=True, padding=True, max_length=args.max_len, return_tensors="pt")
        enc = {k: v.to(device) for k, v in enc.items()}
        logits = mdl(**enc).logits.detach().cpu().numpy()[0]

    if args.neutral_penalty != 0.0:
        logits[2] += args.neutral_penalty  # 'neutral' index 2

    probs = softmax(logits)
    pred_id = int(probs.argmax())
    pred_lbl = ID2LBL[pred_id]

    print("Text:", raw)
    print("Pred:", pred_lbl)
    print("Conf:", f"{float(probs[pred_id]):.6f}")
    print("Probs:")
    for i, name in enumerate(LABELS_5):
        print(f"  {name:15s}: {float(probs[i]):.6f}")

if __name__ == "__main__":
    main()
