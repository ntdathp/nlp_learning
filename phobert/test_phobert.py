#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse, re, numpy as np, torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

LABELS_5 = ["very_negative","negative","neutral","positive","very_positive"]
ID2LBL = {i:l for i,l in enumerate(LABELS_5)}

# ==== same normalize as training ====
EMO_POS = ["ü§©","ü•∞","üòç","‚ù§Ô∏è","üëç","üòé","üëå","‚ú®","üî•","üíØ"]
EMO_NEG = ["üò±","üò°","ü§¨","üí©","üëé","üò§","üòû","üò≠"]

def normalize_text(s: str) -> str:
    s = str(s).strip()
    for e in EMO_POS: s = s.replace(e, " EMO_POS ")
    for e in EMO_NEG: s = s.replace(e, " EMO_NEG ")
    repl = {
        "vl": "r·∫•t", "okeee": "ok", "∆∞ng": "r·∫•t th√≠ch",
        "si√™u si√™u": "r·∫•t", "si√™u th·∫•t v·ªçng": "r·∫•t th·∫•t v·ªçng",
        "m√£i ƒë·ªânh": "r·∫•t t·ªët", "best of best": "r·∫•t t·ªët", "best choice": "r·∫•t t·ªët",
        "ƒë·ªânh c·ªßa ch√≥p": "r·∫•t t·ªët",
    }
    for k,v in repl.items():
        s = re.sub(rf"\b{re.escape(k)}\b", v, s, flags=re.IGNORECASE)
    return s

def maybe_segment(text, use_seg=False):
    if not use_seg: return text
    try:
        from underthesea import word_tokenize
        return word_tokenize(text, format="text")
    except Exception:
        # Kh√¥ng c√≥ underthesea th√¨ b·ªè qua segmentation
        return text

def softmax(x):
    x = x - np.max(x, keepdims=True)
    e = np.exp(x)
    return e / np.sum(e, keepdims=True)

def main():
    ap = argparse.ArgumentParser(description="Quick inference cho 1 c√¢u (PhoBERT 5 l·ªõp).")
    ap.add_argument("--model_dir", default="/home/dat/llm_ws/phobert_5cls_clean",
                    help="Th∆∞ m·ª•c model ƒë√£ save_model()")
    ap.add_argument("--text", required=True, help="C√¢u c·∫ßn d·ª± ƒëo√°n")
    ap.add_argument("--max_len", type=int, default=160)
    ap.add_argument("--use_seg", action="store_true", help="B·∫≠t word segmentation n·∫øu l√∫c train c√≥ b·∫≠t")
    ap.add_argument("--normalize", action="store_true", default=True)
    ap.add_argument("--neutral_penalty", type=float, default=0.0,
                    help="ƒêi·ªÅu ch·ªânh logit l·ªõp neutral (√¢m ƒë·ªÉ ph·∫°t neutral), v√≠ d·ª• -0.2")
    args = ap.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # Load tokenizer & model
    tok = AutoTokenizer.from_pretrained(args.model_dir, use_fast=False)
    mdl = AutoModelForSequenceClassification.from_pretrained(args.model_dir).to(device).eval()

    # Preprocess
    s = args.text
    if args.normalize:
        s = normalize_text(s)
    s = maybe_segment(s, use_seg=args.use_seg)

    # Encode & forward
    with torch.no_grad():
        enc = tok([s], truncation=True, padding=True, max_length=args.max_len, return_tensors="pt")
        enc = {k: v.to(device) for k, v in enc.items()}
        logits = mdl(**enc).logits.detach().cpu().numpy()[0]

    # Optional bias v√†o l·ªõp neutral
    if args.neutral_penalty != 0.0:
        logits[2] += args.neutral_penalty  # index 2 l√† 'neutral'

    probs = softmax(logits)
    pred_id = int(probs.argmax())
    pred_lbl = ID2LBL[pred_id]

    # In k·∫øt qu·∫£
    print("Text:", args.text)
    print("Pred:", pred_lbl)
    print("Conf:", f"{float(probs[pred_id]):.6f}")
    print("Probs:")
    for i, name in enumerate(LABELS_5):
        print(f"  {name:15s}: {float(probs[i]):.6f}")

if __name__ == "__main__":
    main()
